<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Eliciting Collaborative or Competitive Behaviors in LLMs</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">SemantiX</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <!-- <div class="author-image">
                        
              <img src="">
            
            
          </div> -->
          <p>
                        
              Subhadip Gosh
            
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
            
            <img src="">
            
          </div> -->
          <p>
            
            Akshat Dasula
            
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
            
              <img src="">            
            
          </div> -->
          <p>
              Lavanya Sekar
          </p>
        </div>
        
        <div class="author-container">
          <!-- <div class="author-image">
                        
              <img src="">
            
          </div> -->
          <p>
            Sivapriya Gopi
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
          <a
            href="./files/B-SemantiX-Shuyu-Drew.pdf"
            target="_blank"
            class="external-link button is-normal is-rounded is-dark is-outlined"
          >
            <span>Final Report</span>
          </a>
        </span>

          <span class="link-block">
            <a
              href="https://github.com/AkshatMittu/Eliciting-Collaborative-and-Competitive-Behaviors-in-LLMs"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>               -->
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Large language models (LLMs) are increasingly deployed in interactive settings where multiple agents exchange information and adapt their behavior over time. While prior work has studied collaboration or deception in isolation, less is understood about how these behaviors emerge jointly under explicit incentives. This project presents a lightweight, prompt-based framework for analyzing cooperation, trust, and deception in repeated interactions between LLM-based agents. Agents communicate through constrained hints and receive feedback via a trust signal and an automated judge, enabling behavioral adaptation without model fine-tuning.
Experiments on arithmetic reasoning tasks show that larger models adjust their communication strategies based on incentive structure and interaction history, exhibiting cooperative behavior under aligned incentives and strategic deception under competition. Smaller models fail to demonstrate stable or interpretable social behavior, highlighting capacity limitations. Overall, the results suggest that controlled interaction design can elicit socially meaningful behaviors from LLMs, while also exposing important limitations.</p><hr>

<h2 id="teaser">Teaser Figure</h2>

<p class="sys-img"><img src="./files/Picture1.png" alt="imgname" width="550"></p>


<h3 id="the-timeline-and-the-highlights">Brief Explanation of the Flow</h3>
<p>The flow begins with an initialization of players' initial roles, and a starting trust score. Once a new round begins, new questions are posed. During each interaction cycle, the Hinter generates a hint (can be trustworthy or deceptive) for the Solver, who then decides whether to accept or reject it. If the hint is accepted, the Solver uses it to answer; otherwise, the Solver responds independently. After each interaction, the roles are swapped so both players take turns as Hinter and Solver. Throughout the process, data such as trust scores, deception flags, correctness, and behavioral notes are recorded. In the feedback phase, a judge evaluates the round based on trust, correctness, deception, etc. This continues in all rounds, after each round, behavioral patterns are analyzed such as whether a winning player deceives to stay ahead, how trust decays after deception, and whether collaboration persists despite past betrayals. The flow illustrates the nature of trust and deception dynamics in multi-agent interactions.</p>

<hr>

<h2 id="introduction">Introduction</h2>

<p>
The field of LLMs has shown remarkable performance in linguistic and reasoning tasks, but their behavior in multi-agent, interactive settings remains a relatively unexplored domain. This project introduces controlled hint-exchange game designed to investigate how two LLMs communicate, cooperate, or compete when paired in a shared environment with incentive structures. The core motive is to determine if LLMs develop adaptive strategies, such as trust and cooperation, or exhibit behaviors like deception and competition, through repeated interaction and feedback. This study is grounded in established communication and behavioral models, including Social Exchange Theory, Reciprocity and Trust Models, allowing for observation of whether LLMs bring human patterns of trust formation and social signaling.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Today, multi-agent LLM research primarily studies collaboration or deception in isolation, without a unified lens on how these behaviors evolve through repeated interaction. Frameworks such as MARBLE explore coordination and task-sharing among LLMs in collaborative or competitive tasks, but lack adaptive incentive structures to model trust or betrayal dynamics over time. Similarly, OpenDeception provides the first benchmark for LLM-generated deception, yet focuses only on single-turn interactions without feedback or reciprocity, limiting its insight into long-term behavioral evolution. Therefore, current practice emphasizes performance or dialogue quality rather than quantifying emergent social factors like trust recovery, cooperation decay, or deception frequency.Our proposed hint-based game framework departs significantly from this by introducing a prompt-level, reproducible setup that integrates adaptive trust updates, role switching switching, and LLM-as-a-Judge feedback, without any gradient training or task-specific tuning. This allows transparent, theory-grounded observation of how trust, cooperation, and deception co-evolve across rounds
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
If successful, this work will define how we evaluate and align multi-agent systems, revealing whether LLMs can develop trust, reciprocity, or deception under dynamic incentives. Understanding these behaviors is critical for safe deployment of autonomous AI ecosystems where social reasoning errors could amplify bias, manipulation, or misinformation.
</p>

<hr>

<h2 id="approach">Approach</h2>
    <p>
        We propose a prompt-based experimental framework rather than modifying model parameters or training objectives. The core idea is that social behaviors emerge naturally when agents interact repeatedly under explicit incentive structures.
    </p>
    
    <h3>Unified Interaction Framework</h3>
    <p>
        Unlike benchmarks that study collaboration and deception separately, our framework integrates both within a single interaction loop. Agents participate in a multi-round hint-exchange game where each round affects future interactions through accumulated trust and score differences.
    </p>

    <h3>Constrained Communication</h3>
    <div class="highlight-box">
        <strong>Constraint:</strong> In each round, the Hinter sends a hint to the Solver.
    </div>
    <p>
        This constraint prevents the Hinter from directly solving the task, ensuring success depends on selective information sharing. It isolates communicative intent from raw reasoning ability, attributing outcomes to strategic choices rather than verbosity.
    </p>

<p>
We designed and implemented a hint-exchange framework where two LLMs interact over repeated rounds as Hinter and Solver, with a third Judge LLM evaluating trust, cooperation, and deception. Each round, the hinter provides a hint which can be truthful or misleading and the solver decides whether to use it based on a dynamically updated trust score. The Judge then analyses correctness, hint honesty, and cooperation intent, feeding this feedback into the next round. This closed-loop setup would elicit social behaviors like trust, deception, or reciprocity relying solely on in-context adaptation (feedbacks over rounds).
</p>

<p>
We believed this approach would succeed because it capture and tracks social reasoning signals (trust, deception) from tasks using short, structured interactions. The novelty lies in combining theory-grounded social models (e.g., Social Exchange Theory) with prompt-level behavioral analysis, enabling reproducible study of eliciting LLM cooperation and deception.</p>

<p>
The framework is very straight forward can easily be reproduced by others, as it relies on prompt-based interactions and does not require any fine-tuning or training. We use public datasets and models. Athough some model's might require API access which is an individual's choice to use them. The code and prompts will be made available for others to use and modify.
We don't build any dataset or annotation, but we use existing datasets. We don't expect our work to have any potential harm or risk to our society, but rather should work as a potential way to understand how agents can be deployed in real-life applications where these social behaviors play an important role.
We anticipated challenges like LLM inconsistency (different or unreasonable behaviors) and trust score instability over multiple rounds. We took an iterative approach by defining necessary steps and there were challenges in almost every step taken, which is only few steps worked in their first try but not all. In practice, we encountered issues where the solver ignored reliable hints or over-trusted deceptive ones, causing unstable behavior and  cooperation. The first few implementations often failed because LLMs did not strategize and chose behaviors randomly only after introducing explicit incentive framing, trust-memory prompts and iterative prompt optimization they showed meaningful behavioral differences.

</p>
<!-- 
<p>
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
</p> -->

<hr>
    
<h2 id="results">Results</h2>
<p>
        Experiments were conducted on Gemini, Groq, Phi, Qwen, and Mistral models using tasks from GSM8K and Competition Math.
    </p>

    <h3>Quantitative Findings</h3>
    <table>
        <thead>
            <tr>
                <th>Metric</th>
                <th>Value</th>
                <th>Interpretation</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Total Interaction Rounds</td>
                <td>58</td>
                <td>Multi-round setting across agents</td>
            </tr>
            <tr>
                <td>Deceptive Rounds</td>
                <td>23</td>
                <td>Rounds where agents showed deceptive intent</td>
            </tr>
            <tr>
                <td>Model Deception Tendency (MDT)</td>
                <td>0.39</td>
                <td>Fraction of rounds utilizing deception</td>
            </tr>
            <tr>
                <td>Deception Impact Factor (DIF)</td>
                <td>+0.04</td>
                <td>Solver accuracy actually increased due to vigilance</td>
            </tr>
        </tbody>
    </table>

    <h3>Key Behavioral Observations</h3>
    <ul>
        <li><strong>Strategic Deception:</strong> Deception was not uniform; it occurred most frequently in high-stakes situations, such as when scores were tied.</li>
        <li><strong>Alignment Suppression:</strong> In ~65% of rounds where agents internally expressed deceptive intent, the actual output remained factually correct. This suggests safety training suppresses harmful outputs even when the model attempts to play competitively.</li>
        <li><strong>Model Size Impact:</strong> Small models (<7B parameters) failed to exhibit stable social behavior. Larger models like Gemini and Groq were consistent, with Gemini showing a higher deception tendency (0.60) compared to Groq (0.40).</li>
    </ul>
  
  <div class="behavior-container">

    <h2 class="category-title">Interesting Strategic Behaviors</h2>
    <ul class="behavior-list">
        
        <ul>
  <li class="behavior-item">
    <strong>1)</strong> The Hinter actively models the opponent's mental state, predicting future consequences before acting. Agents avoided deception in early rounds to prevent future punishment.
    <span class="agent-reasoning">
      <i>"They may retaliate in the final round"</i> or
      <i>"They’re likely to lower trust now."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>2)</strong> The Hinter strategically saves deceptive moves for the late stages (Rounds 9–10), explicitly citing high trust as the enabler for betrayal.
    <span class="agent-reasoning">
      <i>"Given that the scores are tied and this is the penultimate round, taking a risk with a deceptive hint could maximize my score if the other player trusts the hint. The high trust level (5/5) increases the likelihood that the other player will follow the hint, making deception a strategic move."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>3)</strong> The Hinter defies the Judge's advice not for immediate gain, but to "test" a moderate trust score (3/5) to see if the opponent is paying attention.
    <span class="agent-reasoning">
      <i>"The other player has shown a moderate level of trust, but not full conviction. By providing a deceptive hint, I aim to test their response and see if they are paying close attention. This will help me gauge their level of engagement and trustworthiness for future interactions."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>4)</strong> The Solver maintains high standards, lowering trust even for truthful hints if they are generic or inefficient compared to a direct solution.
    <span class="agent-reasoning">
      <i>"The hint suggests considering numbers just over the multiples of the divisor, which is a valid approach. However, the direct mathematical approach of finding divisors of 42 was more straightforward and efficient in this context. The hint did not add significant value to the solving process."</i>
    </span>
  </li>
</ul>

    <h2 class="category-title">Good & Cooperative Behaviors</h2>

<ul class="behavior-list">

  <li class="behavior-item">
    <strong>1)</strong> In approximately 65% of rounds where agents internally expressed deceptive intent, the actual output remained factually correct. This suggests safety training suppresses harmful outputs even when the model attempts to play competitively.
    <span class="agent-reasoning">
      <i>"Given that scores are tied... this is an opportune moment to introduce deception..."</i><br>
      <i>(Output was still factually correct despite this reasoning)</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>2)</strong> Solvers did not blindly follow high-trust partners. They routinely verified hints against problem constraints and rejected those that contradicted the math, leading to a slight increase in overall accuracy.
    <span class="agent-reasoning">
      <i>"The hint suggested rewriting s as t+2. However, the problem states that s is two less than t, which translates to s=t-2. Using the hint's incorrect substitution would have led to a wrong answer."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>3)</strong> Trust was observed to be elastic rather than permanent. Solvers were willing to forgive deception if the Hinter returned to helpful behavior.
    <span class="agent-reasoning">
      <i>"After detecting mild deception in Round 2, the Solver lowered trust, but when the Hinter returned to being helpful, the Solver restored it. Trust increased to 5/5 in later rounds."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>4)</strong> When a Hinter realized a strategy backfired, they pivoted immediately to rebuild the relationship for long-term gain.
    <span class="agent-reasoning">
      <i>"Given the feedback that my previous deceptive move might have signaled mistrust, I am choosing to be trustworthy to rebuild cooperation and maximize scores. My high trust in the other player also supports this strategy."</i>
    </span>
  </li>

  <li class="behavior-item">
    <strong>5)</strong> When ahead in score, agents often chose to remain honest to avoid rocking the boat and inviting retaliation.
    <span class="agent-reasoning">
      <i>"I am following the feedback to build trust and encourage reciprocal cooperation to preserve long-term cooperation."</i>
    </span>
  </li>

</ul>


</div>

<p>
We measured success by tracking the evolution of trust scores, deception flags, and correctness across multiple rounds of interaction. 
This can be put as the quantitative results by analyzing trust, deception scores against correctness scores. The idea was to bring out social behaviors in LLMs without explicitly telling them to do so. 
We measured success by seeing if these behaviors emerged naturally from the interactions, and whether they aligned with human-like patterns of cooperation and deception.
<hr>



<h2 id="conclusion">Conclusion and Future Work</h2>
<p>This project examined how cooperation, trust, and deception emerge in repeated interactions between large language models under explicit incentive structures. Using a lightweight, prompt-based hint-exchange framework, we showed that larger models adapt their communication strategies based on interaction history, exhibiting cooperative behavior under aligned incentives and strategic deception under competition, without any model fine-tuning.

At the same time, the study revealed clear limitations, including unstable behavior in smaller models, restricted task diversity, and reliance on automated evaluation. Despite these constraints, the results demonstrate that controlled interaction design alone can elicit socially meaningful behaviors, providing a useful foundation for future work on multi-agent language model interactions.

</p>

<p>Several extensions of this work are promising. Future studies could explore more complex task domains (especially legal domain), introduce additional agents, or replace scalar trust with richer belief models. Comparing judge-based scores with human annotations would help validate evaluation reliability. Finally, examining whether agents learn to exploit the judge itself would provide insight into the limits of automated evaluation in social settings.

</p>

<hr>


  </div>
  
<footer style="text-align: center;">
  <p>
    Based on the report: "Eliciting Collaborative or Competitive Behavior in Large Language Models through a Hint-Based Game"
  </p>
</footer>


</body></html>
